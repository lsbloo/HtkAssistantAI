from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from pydantic import BaseModel
from langchain.schema import (
    AIMessage,
    BaseMessage,
    ChatMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
)
from ..model.roles import RoleType
import warnings
from ..base.htk_base import HtkClientBase
from core.setup.config_environment import environments_config
from core.prompts.prompts_manager import HtkPromptsModelInitializerManager
from core.log.htk_logger import HtkApplicationLogger


"""
This module provides the implementation of the `HtkGroqClient` class, which serves as a wrapper for interacting 
with the Groq language model using the `langchain_groq` library. It includes functionality for managing conversation 
history and generating responses based on user input.
Classes:
---------
- HtkGroqClient:
    A client for interacting with the Groq language model. It supports chat-based interactions and maintains 
    conversation history using a buffer memory.
- Message:
    A simple Pydantic model for representing a message with content.
Constants:
----------
- GROQ_CLIENT_DEFAULT_ROLE:
    The default role assigned to the client (e.g., "user").
- GROQ_CLIENT_DEFAULT_MODEL:
    The default model used for the Groq client (e.g., "llama-3.3-70b-versatile").
Methods:
--------
- HtkGroqClient.__init__(api_key=None):
    Initializes the Groq client with the provided API key, default model, and memory buffer.
- HtkGroqClient.generate_text_with_prompt(prompt):
    Placeholder method for generating text based on a given prompt.
- HtkGroqClient.chat(message) -> str:
    Sends a message to the Groq client, retrieves the response, and updates the conversation history.
- HtkGroqClient.initialize_with_roles(system_message, user_message):
    Placeholder method for initializing the client with specific roles.
Dependencies:
-------------
- langchain_groq.ChatGroq: 
    The Groq client for interacting with the language model.
- langchain.memory.ConversationBufferMemory:
    A memory buffer for storing conversation history.
- langchain.prompts.ChatPromptTemplate:
    Utility for creating chat prompts.
- langchain.schema.SystemMessage, langchain.schema.HumanMessage:
    Classes for representing system and human messages in the conversation.
- pydantic.BaseModel:
    Base class for creating data models.
"""
GROQ_CLIENT_DEFAULT_ROLE = "user"
GROQ_CLIENT_DEFAULT_MODEL = "llama-3.3-70b-versatile"


class HtkGroqClient(HtkClientBase):
    def __init__(self, api_key=None):
        self._api_key = api_key
        if not self._api_key:
            raise ValueError(
                "API key for Groq is not set in the environment configuration."
            )

        self._logger = HtkApplicationLogger()
        self._prompt_initializer_manager = HtkPromptsModelInitializerManager()

        self._logger.log("Driver Groq is started!")
        self.client = ChatGroq(
            groq_api_key=api_key,
            model_name=GROQ_CLIENT_DEFAULT_MODEL,
            temperature=0.7,
            max_tokens=1000,
        )

        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            try:
                self.memory = ConversationBufferMemory(
                    memory_key="chat_history", return_messages=True
                )
            except Exception as e:
                self._logger.log("Error initializing ConversationBufferMemory:")
                print("Error initializing ConversationBufferMemory:", str(e))
                self.memory = None

    def update_client(self):
        self._logger.log("Driver Groq is updated!")
        default_configs = self._prompt_initializer_manager.find_model_config_by_name(
            "Groq"
        )
        self._logger.log(
            f"Parameters: model_name: {default_configs.model_name} - temperature: {default_configs.temperature} - max_tokens: {default_configs.max_token} - max_retries: {default_configs.max_retries} - n: {default_configs.n}"
        )
        self.client = ChatGroq(
            groq_api_key=self._api_key,
            model_name=default_configs.model_name,
            temperature=default_configs.temperature,
            max_tokens=default_configs.max_token,
            max_retries=default_configs.max_retries,
            n=default_configs.n,
        )

    """
        Handles a chat interaction by sending a message to the client and managing the chat history.
        Args:
            message (str): The input message from the user.
        Returns:
            str: The response content generated by the client.
        Workflow:
            1. Wraps the input message in a `Message` object.
            2. Retrieves the current chat history from memory.
            3. Prepends a system message to the chat history to establish the assistant's role.
            4. Appends the user's input message to the formatted history.
            5. Sends the formatted history to the client for processing.
            6. Saves the input and output context to memory.
            7. Returns the client's response as a string.
    """

    def chat(self, message, additionalContext=None) -> str:
        input_message = Message(content=message)

        if additionalContext != None:
            self.memory.chat_memory.add_message(
                SystemMessage(content=additionalContext)
            )

        chat_history = self.memory.chat_memory.messages

        system_message = SystemMessage(content="You are a helpful assistant.")
        formatted_history = (
            [system_message]
            + chat_history
            + [HumanMessage(content=input_message.content)]
        )

        response = self.client.invoke(input=formatted_history)

        self.memory.save_context(
            {"input": input_message.content}, {"output": str(response.content)}
        )

        return str(response.content)

    def chat_with_roles(self, message, role, additionalContext=None) -> str:
        input_message = Message(content=message)

        if additionalContext != None:
            self.memory.chat_memory.add_message(
                SystemMessage(content=additionalContext)
            )

        chat_history = self.memory.chat_memory.messages

        if role == RoleType.SYSTEM:
            system_message = SystemMessage(content="You are a system.")
        elif role == RoleType.ASSISTANT:
            system_message = AIMessage(content="You are the assistant.")
        else:
            system_message = HumanMessage(content="You are the user.")

        formatted_history = (
            [system_message]
            + chat_history
            + [HumanMessage(content=input_message.content)]
        )

        response = self.client.invoke(input=formatted_history)

        self.memory.save_context(
            {"input": input_message.content}, {"output": str(response.content)}
        )

        return str(response.content)


class Message(BaseModel):
    content: str


class HtkGroqInitializer:
    _instance = None
    _instance_groq = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def getInstanceGroq(self):
        if self._instance_groq is None:
            self._instance_groq = HtkGroqClient(
                environments_config.get("HTK_ASSISTANT_API_KEY_LLM_GROQ")
            )
        self._instance_groq.update_client()
        return self._instance_groq
