
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from pydantic import BaseModel
from langchain.schema import AIMessage, BaseMessage, ChatMessage, FunctionMessage, HumanMessage, SystemMessage
from ..model.roles import RoleType
import warnings
from ..base.htk_base import HtkClientBase


"""
This module provides the implementation of the `HtkGroqClient` class, which serves as a wrapper for interacting 
with the Groq language model using the `langchain_groq` library. It includes functionality for managing conversation 
history and generating responses based on user input.
Classes:
---------
- HtkGroqClient:
    A client for interacting with the Groq language model. It supports chat-based interactions and maintains 
    conversation history using a buffer memory.
- Message:
    A simple Pydantic model for representing a message with content.
Constants:
----------
- GROQ_CLIENT_DEFAULT_ROLE:
    The default role assigned to the client (e.g., "user").
- GROQ_CLIENT_DEFAULT_MODEL:
    The default model used for the Groq client (e.g., "llama-3.3-70b-versatile").
Methods:
--------
- HtkGroqClient.__init__(api_key=None):
    Initializes the Groq client with the provided API key, default model, and memory buffer.
- HtkGroqClient.generate_text_with_prompt(prompt):
    Placeholder method for generating text based on a given prompt.
- HtkGroqClient.chat(message) -> str:
    Sends a message to the Groq client, retrieves the response, and updates the conversation history.
- HtkGroqClient.initialize_with_roles(system_message, user_message):
    Placeholder method for initializing the client with specific roles.
Dependencies:
-------------
- langchain_groq.ChatGroq: 
    The Groq client for interacting with the language model.
- langchain.memory.ConversationBufferMemory:
    A memory buffer for storing conversation history.
- langchain.prompts.ChatPromptTemplate:
    Utility for creating chat prompts.
- langchain.schema.SystemMessage, langchain.schema.HumanMessage:
    Classes for representing system and human messages in the conversation.
- pydantic.BaseModel:
    Base class for creating data models.
"""
GROQ_CLIENT_DEFAULT_ROLE = "user"
GROQ_CLIENT_DEFAULT_MODEL = "llama-3.3-70b-versatile"

class HtkGroqClient(HtkClientBase):
    def __init__(self, api_key=None):
        api_key = api_key
        if not api_key:
            raise ValueError("API key for Groq is not set in the environment configuration.")
        
        self.client = ChatGroq(groq_api_key=api_key, 
                               model_name=GROQ_CLIENT_DEFAULT_MODEL,
                               temperature=0.7, 
                               max_tokens=1000
                               )
        
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            try: 
                self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            except Exception as e:
                print("Error initializing ConversationBufferMemory:", str(e))
                self.memory = None
                
    """
        Handles a chat interaction by sending a message to the client and managing the chat history.
        Args:
            message (str): The input message from the user.
        Returns:
            str: The response content generated by the client.
        Workflow:
            1. Wraps the input message in a `Message` object.
            2. Retrieves the current chat history from memory.
            3. Prepends a system message to the chat history to establish the assistant's role.
            4. Appends the user's input message to the formatted history.
            5. Sends the formatted history to the client for processing.
            6. Saves the input and output context to memory.
            7. Returns the client's response as a string.
    """
    def chat(self, message) -> str:
        input_message = Message(content=message)
        
        chat_history = self.memory.chat_memory.messages
        
        system_message = SystemMessage(content='You are a helpful assistant.')
        formatted_history = [system_message] + chat_history + [HumanMessage(content=input_message.content)]
        
        response = self.client.invoke(input = formatted_history)
        
        self.memory.save_context({"input": input_message.content}, {"output": str(response.content)})
            
        return str(response.content)
    
    def chat_with_roles(self, message, role) -> str:
        input_message = Message(content=message)
        
        chat_history = self.memory.chat_memory.messages
        
        if(role == RoleType.SYSTEM):
            system_message = SystemMessage(content='You are a system.')
        elif(role == RoleType.ASSISTANT):
            system_message = AIMessage(content='You are the assistant.')
        else:
            system_message = HumanMessage(content='You are the user.')
            
        formatted_history = [system_message] + chat_history + [HumanMessage(content=input_message.content)]
        
        response = self.client.invoke(input = formatted_history)
        
        self.memory.save_context({"input": input_message.content}, {"output": str(response.content)})
            
        return str(response.content)
       
   

class Message(BaseModel):
    content: str